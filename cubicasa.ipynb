{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy, JaccardIndex\n",
    "\n",
    "# CubiCasa\n",
    "from floortrans.loaders import FloorplanSVG\n",
    "from floortrans.loaders.augmentations import (RandomCropToSizeTorch,\n",
    "                                              ResizePaddedTorch,\n",
    "                                              Compose,\n",
    "                                              DictToTensor,\n",
    "                                              ColorJitterTorch,\n",
    "                                              RandomRotations)\n",
    "\n",
    "# Own modules\n",
    "from models.deeplabv3plus import DeepLabV3Plus\n",
    "from evaluation_metrics import Metrics, timer\n",
    "\n",
    "# Release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory has been released.\")\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")\n",
    "\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "aug = Compose([transforms.RandomChoice([RandomCropToSizeTorch(data_format='dict', size=IMAGE_SIZE),\n",
    "                                            ResizePaddedTorch((0, 0), data_format='dict', size=IMAGE_SIZE)]),\n",
    "                                        RandomRotations(format='cubi'),\n",
    "                                        DictToTensor(),\n",
    "                                        ColorJitterTorch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/cubicasa5k/'\n",
    "TRAIN_PATH = 'train.txt'\n",
    "VAL_PATH = 'val.txt'\n",
    "FORMAT = 'lmdb'\n",
    "\n",
    "\n",
    "train_set = FloorplanSVG(DATA_PATH, TRAIN_PATH, format=FORMAT, augmentations=aug)\n",
    "\n",
    "# Use this in the meantime to prevent kernel dying\n",
    "# train_set = Subset(full_train_set, list(range(1000)))\n",
    "\n",
    "val_set = FloorplanSVG(DATA_PATH, VAL_PATH, format=FORMAT, augmentations=DictToTensor())\n",
    "\n",
    "print('Train set size:', len(train_set))\n",
    "print('Validation set size:', len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set[np.random.randint(0, len(train_set))]\n",
    "print('Image shape:', sample['image'].shape)\n",
    "print('Label shape:', sample['label'].shape)\n",
    "\n",
    "print('\\nLabel shape (rooms): ', sample['label'][0].shape)\n",
    "print('Label shape (icons): ', sample['label'][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image: ', sample['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Label: ', sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to [0, 255] range\n",
    "tensor_image = sample['image'] * 255.0\n",
    "\n",
    "np_image = tensor_image.numpy().astype(np.uint8)  # Convert to unsigned 8-bit integer\n",
    "\n",
    "# Transpose to [H, W, 3] from [3, H, W]\n",
    "np_image = np.transpose(np_image, (1, 2, 0))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns\n",
    "\n",
    "# Input image\n",
    "axes[0].imshow(np_image)\n",
    "axes[0].axis('off')  # Remove the axes\n",
    "axes[0].set_title('Input Image')\n",
    "\n",
    "# Room segmentation map\n",
    "axes[1].imshow(sample['label'][0])\n",
    "axes[1].axis('off')  # Remove the axes\n",
    "axes[1].set_title('Room Labels')\n",
    "\n",
    "# Icon segmentation map\n",
    "axes[2].imshow(sample['label'][1])\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Icon Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    shuffle=True, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=1, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'Length of train dataloader: {len(train_loader)} batches of size {BATCH_SIZE}')\n",
    "print(f'Length of val dataloader: {len(val_loader)} batches of size {BATCH_SIZE}')\n",
    "\n",
    "batch_sample = next(iter(train_loader))\n",
    "print('\\nBatch image shape: ', batch_sample['image'].shape)\n",
    "print('Batch label shape: ', batch_sample['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "For reference, here are the 23 classes:  \n",
    "\n",
    "- **Rooms (12):** \"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"  \n",
    "\n",
    "- **Icons (11):** \"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\", \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLabV3Plus(backbone='mobilenetv2', attention=False)\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, IMAGE_SIZE[0], IMAGE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying output shapes\n",
    "sample_input = torch.randn(16, 3, 256, 256)\n",
    "\n",
    "room_output, icon_output = model(sample_input.to(device))\n",
    "\n",
    "print(\"Room Output Shape: \", room_output.shape)  # Expected: [16, 12, 256, 256]\n",
    "print(\"Icon Output Shape: \", icon_output.shape)  # Expected: [16, 11, 256, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multitask_loss(room_output, icon_output, room_labels, icon_labels, alpha=1.0, beta=1.0):\n",
    "#     \"\"\"\n",
    "#     Compute the multitask loss for room and icon segmentation.\n",
    "    \n",
    "#     Args:\n",
    "#         room_output: Model's room segmentation output.\n",
    "#         icon_output: Model's icon segmentation output.\n",
    "#         room_labels: Ground truth for room segmentation.\n",
    "#         icon_labels: Ground truth for icon segmentation.\n",
    "#         alpha: Weight for room segmentation loss.\n",
    "#         beta: Weight for icon segmentation loss.\n",
    "    \n",
    "#     Returns:\n",
    "#         total_loss: Combined loss for room and icon segmentation.\n",
    "#     \"\"\"\n",
    "\n",
    "#     room_labels = room_labels.long()\n",
    "#     icon_labels = icon_labels.long()\n",
    "\n",
    "#     # Cross-Entropy Loss for room and icon segmentation\n",
    "#     room_loss = F.cross_entropy(room_output, room_labels)\n",
    "#     icon_loss = F.cross_entropy(icon_output, icon_labels)\n",
    "    \n",
    "#     # Combine losses using weights\n",
    "#     total_loss = alpha * room_loss + beta * icon_loss\n",
    "#     return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    def __init__(self, task_num):\n",
    "        super(MultiTaskLossWrapper, self).__init__()\n",
    "        # Learnable log of the task uncertainties (log_sigma for stability)\n",
    "        self.log_sigma_room = nn.Parameter(torch.tensor(0.0))  # Room segmentation\n",
    "        self.log_sigma_icon = nn.Parameter(torch.tensor(0.0))  # Icon segmentation\n",
    "\n",
    "    def forward(self, room_output, icon_output, room_labels, icon_labels):\n",
    "        \"\"\"\n",
    "        Compute the multi-task loss with uncertainty-driven weighting for room and icon segmentation.\n",
    "\n",
    "        Args:\n",
    "            room_output: Model's room segmentation output.\n",
    "            icon_output: Model's icon segmentation output.\n",
    "            room_labels: Ground truth for room segmentation.\n",
    "            icon_labels: Ground truth for icon segmentation.\n",
    "\n",
    "        Returns:\n",
    "            total_loss: Combined loss for room and icon segmentation with uncertainty weighting.\n",
    "        \"\"\"\n",
    "        room_labels = room_labels.long()\n",
    "        icon_labels = icon_labels.long()\n",
    "\n",
    "        # Cross-Entropy Loss for room and icon segmentation\n",
    "        room_loss = F.cross_entropy(room_output, room_labels)\n",
    "        icon_loss = F.cross_entropy(icon_output, icon_labels)\n",
    "\n",
    "        # Uncertainty-weighted loss\n",
    "        # Loss for each task is scaled by exp(-2 * log_sigma) and regularized by log_sigma\n",
    "        loss_room = (1 / (2 * torch.exp(self.log_sigma_room))) * room_loss + self.log_sigma_room\n",
    "        loss_icon = (1 / (2 * torch.exp(self.log_sigma_icon))) * icon_loss + self.log_sigma_icon\n",
    "\n",
    "        # Total loss is the sum of the two uncertainty-weighted losses\n",
    "        total_loss = loss_room + loss_icon\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 100\n",
    "\n",
    "# CRITERION = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# initial_lr = 0.001\n",
    "# OPTIMIZER = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.95, weight_decay=1e-4, nesterov=True)\n",
    "\n",
    "# # Poly learning rate policy (used in DeepLabV3+ paper)\n",
    "# class PolyLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "#     def __init__(self, optimizer, max_iters, power=0.9, last_epoch=-1):\n",
    "#         self.max_iters = max_iters\n",
    "#         self.power = power\n",
    "#         super(PolyLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "#     def get_lr(self):\n",
    "#         return [base_lr * (1 - self.last_epoch / self.max_iters) ** self.power for base_lr in self.base_lrs]\n",
    "\n",
    "# max_iters = EPOCHS * len(train_loader)\n",
    "# SCHEDULER = PolyLR(OPTIMIZER, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "CRITERION = MultiTaskLossWrapper(task_num=2)\n",
    "\n",
    "def timer(start_time = None): \n",
    "    return time.time() if start_time == None else time.time() - start_time\n",
    "\n",
    "\n",
    "def train_evaluate(model, \n",
    "                   train_loader, \n",
    "                   val_loader, \n",
    "                   device,\n",
    "                   loss_fn,\n",
    "                   optimizer, \n",
    "                   epochs,\n",
    "                   early_stop_threshold=15,\n",
    "                   save_prefix='deeplabv3plus',\n",
    "                   save_path='saved_models'):\n",
    "    \n",
    "    # Store results, to be returned\n",
    "    train_loss_list = []\n",
    "    train_room_cpa_list = []\n",
    "    train_room_mpa_list = []\n",
    "    train_room_miou_list = []\n",
    "    train_room_fwiou_list = []\n",
    "    train_icon_cpa_list = []\n",
    "    train_icon_mpa_list = []\n",
    "    train_icon_miou_list = []\n",
    "    train_icon_fwiou_list = []\n",
    "\n",
    "    val_loss_list = []\n",
    "    val_room_cpa_list = []\n",
    "    val_room_mpa_list = []\n",
    "    val_room_miou_list = []\n",
    "    val_room_fwiou_list = []\n",
    "    val_icon_cpa_list = []\n",
    "    val_icon_mpa_list = []\n",
    "    val_icon_miou_list = []\n",
    "    val_icon_fwiou_list = []\n",
    "\n",
    "    # Training metrics\n",
    "    train_room_cpa = Accuracy(task='multiclass', num_classes=12, average=None).to(device)\n",
    "    train_room_mpa = Accuracy(task='multiclass', num_classes=12, average='macro').to(device)\n",
    "    train_room_miou = JaccardIndex(task='multiclass', num_classes=12, average='macro').to(device)\n",
    "    train_room_fwiou = JaccardIndex(task='multiclass', num_classes=12, average='weighted').to(device)\n",
    "\n",
    "    train_icon_cpa = Accuracy(task='multiclass', num_classes=11, average=None).to(device)\n",
    "    train_icon_mpa = Accuracy(task='multiclass', num_classes=11, average='macro').to(device)\n",
    "    train_icon_miou = JaccardIndex(task='multiclass', num_classes=11, average='macro').to(device)\n",
    "    train_icon_fwiou = JaccardIndex(task='multiclass', num_classes=11, average='weighted').to(device)\n",
    "\n",
    "    # Validation metrics\n",
    "    val_room_cpa = Accuracy(task='multiclass', num_classes=12, average=None).to(device)\n",
    "    val_room_mpa = Accuracy(task='multiclass', num_classes=12, average='macro').to(device)\n",
    "    val_room_miou = JaccardIndex(task='multiclass', num_classes=12, average='macro').to(device)\n",
    "    val_room_fwiou = JaccardIndex(task='multiclass', num_classes=12, average='weighted').to(device)\n",
    "\n",
    "    val_icon_cpa = Accuracy(task='multiclass', num_classes=11, average=None).to(device)\n",
    "    val_icon_mpa = Accuracy(task='multiclass', num_classes=11, average='macro').to(device)\n",
    "    val_icon_miou = JaccardIndex(task='multiclass', num_classes=11, average='macro').to(device)\n",
    "    val_icon_fwiou = JaccardIndex(task='multiclass', num_classes=11, average='weighted').to(device)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    not_improving = 0\n",
    "    \n",
    "    # Save models in this directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Start timer\n",
    "    train_start = timer()\n",
    "    print('Start training process...')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = timer()\n",
    "        \n",
    "        # Training loop\n",
    "        print(f'Epoch {epoch} train process started...')\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            images = batch['image'].to(device)\n",
    "            room_labels = batch['label'][:, 0].to(device)\n",
    "            icon_labels = batch['label'][:, 1].to(device)\n",
    "\n",
    "            # Reset gradients since PyTorch accumulates previous gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            room_output, icon_output = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(room_output, icon_output, room_labels, icon_labels)\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            train_room_cpa.update(room_output, room_labels)\n",
    "            train_room_mpa.update(room_output, room_labels)\n",
    "            train_room_miou.update(room_output, room_labels)\n",
    "            train_room_fwiou.update(room_output, room_labels)\n",
    "\n",
    "            train_icon_cpa.update(icon_output, icon_labels)\n",
    "            train_icon_mpa.update(icon_output, icon_labels)\n",
    "            train_icon_miou.update(icon_output, icon_labels)\n",
    "            train_icon_fwiou.update(icon_output, icon_labels)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_room_cpa_value = train_room_cpa.compute()\n",
    "        train_room_mpa_value = train_room_mpa.compute().item()\n",
    "        train_room_miou_value = train_room_miou.compute().item()\n",
    "        train_room_fwiou_value = train_room_fwiou.compute().item()\n",
    "\n",
    "        train_icon_cpa_value = train_icon_cpa.compute()\n",
    "        train_icon_mpa_value = train_icon_mpa.compute().item()\n",
    "        train_icon_miou_value = train_icon_miou.compute().item()\n",
    "        train_icon_fwiou_value = train_icon_fwiou.compute().item()\n",
    "\n",
    "        # Reset metrics\n",
    "        train_room_cpa.reset()\n",
    "        train_room_mpa.reset()\n",
    "        train_room_miou.reset()\n",
    "        train_room_fwiou.reset()\n",
    "\n",
    "        train_icon_cpa.reset()\n",
    "        train_icon_mpa.reset()\n",
    "        train_icon_miou.reset()\n",
    "        train_icon_fwiou.reset()\n",
    "\n",
    "        \n",
    "        # Validation loop\n",
    "        print(f'Epoch {epoch} validation process started...')\n",
    "        model.eval()\n",
    "\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader):\n",
    "                images = batch['image'].to(device)\n",
    "                room_labels = batch['label'][:, 0].to(device)\n",
    "                icon_labels = batch['label'][:, 1].to(device)\n",
    "\n",
    "                # Get model predictions\n",
    "                room_output, icon_output = model(images)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(room_output, icon_output, room_labels, icon_labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "                # Update metrics\n",
    "                val_room_cpa.update(room_output, room_labels)\n",
    "                val_room_mpa.update(room_output, room_labels)\n",
    "                val_room_miou.update(room_output, room_labels)\n",
    "                val_room_fwiou.update(room_output, room_labels)\n",
    "\n",
    "                val_icon_cpa.update(icon_output, icon_labels)\n",
    "                val_icon_mpa.update(icon_output, icon_labels)\n",
    "                val_icon_miou.update(icon_output, icon_labels)\n",
    "                val_icon_fwiou.update(icon_output, icon_labels)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_room_cpa_value = val_room_cpa.compute()\n",
    "        val_room_mpa_value = val_room_mpa.compute().item()\n",
    "        val_room_miou_value = val_room_miou.compute().item()\n",
    "        val_room_fwiou_value = val_room_fwiou.compute().item()\n",
    "\n",
    "        val_icon_cpa_value = val_icon_cpa.compute()\n",
    "        val_icon_mpa_value = val_icon_mpa.compute().item()\n",
    "        val_icon_miou_value = val_icon_miou.compute().item()\n",
    "        val_icon_fwiou_value = val_icon_fwiou.compute().item()\n",
    "\n",
    "        # Reset metrics\n",
    "        val_room_cpa.reset()\n",
    "        val_room_mpa.reset()\n",
    "        val_room_miou.reset()\n",
    "        val_room_fwiou.reset()\n",
    "\n",
    "        val_icon_cpa.reset()\n",
    "        val_icon_mpa.reset()\n",
    "        val_icon_miou.reset()\n",
    "        val_icon_fwiou.reset()\n",
    "\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch} train process is completed.')\n",
    "        print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print(f'\\nEpoch {epoch} train process results:\\n')\n",
    "\n",
    "        print(f'Train Time: {timer(epoch_start):.3f} secs')\n",
    "\n",
    "        # Combine room and icon metrics for training, including loss\n",
    "        print(f'Train Room - Loss: {epoch_train_loss / len(train_loader):.3f}, MPA: {train_room_mpa_value:.3f}, mIOU: {train_room_miou_value:.3f}, fwIOU: {train_room_fwiou_value:.3f}')\n",
    "        print(f'Train Icon - Loss: {epoch_train_loss / len(train_loader):.3f}, MPA: {train_icon_mpa_value:.3f}, mIOU: {train_icon_miou_value:.3f}, fwIOU: {train_icon_fwiou_value:.3f}')\n",
    "\n",
    "        print(f'\\nVal process results:')\n",
    "\n",
    "        # Combine room and icon metrics for validation, including loss\n",
    "        print(f'Val Room - Loss: {epoch_val_loss / len(val_loader):.3f}, MPA: {val_room_mpa_value:.3f}, mIOU: {val_room_miou_value:.3f}, fwIOU: {val_room_fwiou_value:.3f}')\n",
    "        print(f'Val Icon - Loss: {epoch_val_loss / len(val_loader):.3f}, MPA: {val_icon_mpa_value:.3f}, mIOU: {val_icon_miou_value:.3f}, fwIOU: {val_icon_fwiou_value:.3f}')\n",
    "\n",
    "\n",
    "        # Append results\n",
    "        train_loss_list.append(epoch_train_loss / len(train_loader))\n",
    "        train_room_cpa_list.append(train_room_cpa_value)\n",
    "        train_room_mpa_list.append(train_room_mpa_value)\n",
    "        train_room_miou_list.append(train_room_miou_value)\n",
    "        train_room_fwiou_list.append(train_room_fwiou_value)\n",
    "        train_icon_cpa_list.append(train_icon_cpa_value)\n",
    "        train_icon_mpa_list.append(train_icon_mpa_value)\n",
    "        train_icon_miou_list.append(train_icon_miou_value)\n",
    "        train_icon_fwiou_list.append(train_icon_fwiou_value)\n",
    "\n",
    "        val_loss_list.append(epoch_val_loss / len(val_loader))\n",
    "        val_room_cpa_list.append(val_room_cpa_value)\n",
    "        val_room_mpa_list.append(val_room_mpa_value)\n",
    "        val_room_miou_list.append(val_room_miou_value)\n",
    "        val_room_fwiou_list.append(val_room_fwiou_value)\n",
    "        val_icon_cpa_list.append(val_icon_cpa_value)\n",
    "        val_icon_mpa_list.append(val_icon_mpa_value)\n",
    "        val_icon_miou_list.append(val_icon_miou_value)\n",
    "        val_icon_fwiou_list.append(val_icon_fwiou_value)\n",
    "\n",
    "        # Save model if validation loss is improved\n",
    "        if (epoch_val_loss / len(val_loader)) < best_loss:\n",
    "            print(f'\\nLoss decreased from {best_loss:.3f} to {(epoch_val_loss / len(val_loader)):.3f}!')\n",
    "            best_loss = (epoch_val_loss / len(val_loader))\n",
    "\n",
    "            not_improving = 0 # Reset counter\n",
    "\n",
    "            print('Saving the model with the best loss value...')\n",
    "            torch.save(model.state_dict(), f'{save_path}/{save_prefix}.pt')\n",
    "        \n",
    "        else:\n",
    "            not_improving += 1\n",
    "            print(f'\\nLoss did not decrease for {not_improving} epoch(s)!')\n",
    "\n",
    "            if not_improving == early_stop_threshold:\n",
    "                print(f'Stopping training process because loss did not decrease for {early_stop_threshold} epochs!')\n",
    "                break\n",
    "        \n",
    "        print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    print(f'Train process is completed in {(timer(train_start)) / 60:.3f} minutes.')\n",
    "\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_loss_list,\n",
    "        'train_room_cpa': train_room_cpa_list,\n",
    "        'train_room_mpa': train_room_mpa_list,\n",
    "        'train_room_miou': train_room_miou_list,\n",
    "        'train_room_fwiou': train_room_fwiou_list,\n",
    "        'train_icon_cpa': train_icon_cpa_list,\n",
    "        'train_icon_mpa': train_icon_mpa_list,\n",
    "        'train_icon_miou': train_icon_miou_list,\n",
    "        'train_icon_fwiou': train_icon_fwiou_list,\n",
    "        'val_loss': val_loss_list,\n",
    "        'val_room_cpa': val_room_cpa_list,\n",
    "        'val_room_mpa': val_room_mpa_list,\n",
    "        'val_room_miou': val_room_miou_list,\n",
    "        'val_room_fwiou': val_room_fwiou_list,\n",
    "        'val_icon_cpa': val_icon_cpa_list,\n",
    "        'val_icon_mpa': val_icon_mpa_list,\n",
    "        'val_icon_miou': val_icon_miou_list,\n",
    "        'val_icon_fwiou': val_icon_fwiou_list\n",
    "    }\n",
    "\n",
    "\n",
    "# Run training and validation process\n",
    "history = train_evaluate(model,\n",
    "                         train_loader,\n",
    "                         val_loader,\n",
    "                         device,\n",
    "                         CRITERION,\n",
    "                         OPTIMIZER,\n",
    "                         EPOCHS,\n",
    "                         save_prefix=f'deeplabv3plus_{model.backbone_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Plot():\n",
    "#     def __init__(self, results):\n",
    "#         self.results = results\n",
    "\n",
    "#         self.visualize(metric1=\"tr_iou\", \n",
    "#                        metric2=\"val_iou\", \n",
    "#                        label1=\"Train IoU\",\n",
    "#                        label2 =\"Validation IoU\", \n",
    "#                        title=\"Mean Intersection Over Union Learning Curve\", \n",
    "#                        ylabel=\"mIoU Score\")\n",
    "\n",
    "#         self.visualize(metric1=\"tr_pa\", \n",
    "#                        metric2=\"val_pa\", \n",
    "#                        label1=\"Train PA\",\n",
    "#                        label2=\"Validation PA\", \n",
    "#                        title=\"Pixel Accuracy Learning Curve\", \n",
    "#                        ylabel=\"PA Score\")\n",
    "\n",
    "#         self.visualize(metric1=\"tr_loss\", \n",
    "#                        metric2=\"val_loss\", \n",
    "#                        label1=\"Train Loss\",\n",
    "#                        label2=\"Validation Loss\", \n",
    "#                        title=\"Loss Learning Curve\", \n",
    "#                        ylabel=\"Loss Value\")\n",
    "\n",
    "#     def plot(self, metric, label): \n",
    "#         plt.plot(self.results[metric], label=label)\n",
    "\n",
    "#     def decorate(self, ylabel, title): \n",
    "#         plt.title(title)\n",
    "#         plt.xlabel(\"Epochs\")\n",
    "#         plt.ylabel(ylabel)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "#     def visualize(self, metric1, metric2, label1, label2, title, ylabel):\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         self.plot(metric1, label1)\n",
    "#         self.plot(metric2, label2)\n",
    "#         self.decorate(ylabel, title)\n",
    "\n",
    "\n",
    "# Plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Test():\n",
    "#     def __init__(self, model, test_loader, loss_fn, device):\n",
    "#         self.model = model\n",
    "#         self.test_loader = test_loader\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.device = device\n",
    "    \n",
    "#     def run(self):\n",
    "#         self.model.eval()\n",
    "#         test_loss = 0\n",
    "#         test_iou = 0\n",
    "#         test_pixel_acc = 0\n",
    "#         test_len = len(self.test_loader)\n",
    "\n",
    "#         imgs = []\n",
    "#         gts = []\n",
    "#         preds = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(self.test_loader):\n",
    "#                 imgs_batch = batch['image']\n",
    "#                 gts_batch = batch['label']\n",
    "#                 imgs_batch, gts_batch = imgs_batch.to(self.device), gts_batch.to(self.device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 preds_batch = self.model(imgs_batch)\n",
    "                \n",
    "#                 # Calculate metrics\n",
    "#                 metrics = Metrics(preds_batch, gts_batch, self.loss_fn)\n",
    "#                 test_loss += metrics.loss().item()\n",
    "#                 test_iou += metrics.mIOU()\n",
    "#                 test_pixel_acc += metrics.PixelAcc()\n",
    "\n",
    "#                 # Collect data for visualization\n",
    "#                 preds_batch = torch.argmax(preds_batch, dim=1)\n",
    "#                 imgs.extend(imgs_batch.cpu())\n",
    "#                 gts.extend(gts_batch.cpu())\n",
    "#                 preds.extend(preds_batch.cpu())\n",
    "\n",
    "#         # Calculate average metrics\n",
    "#         test_loss /= test_len\n",
    "#         test_iou /= test_len\n",
    "#         test_pixel_acc /= test_len\n",
    "\n",
    "#         return imgs, gts, preds, test_loss, test_iou, test_pixel_acc\n",
    "\n",
    "\n",
    "# test = Test(model, test_loader, CRITERION, device)\n",
    "# imgs, gts, preds, test_loss, test_iou, test_pixel_acc = test.run()\n",
    "\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test mIoU: {test_iou:.4f}\")\n",
    "# print(f\"Test PA: {test_pixel_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
