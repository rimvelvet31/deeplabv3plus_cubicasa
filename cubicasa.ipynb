{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import RandomChoice\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from floortrans.loaders import FloorplanSVG\n",
    "from floortrans.loaders.augmentations import (RandomCropToSizeTorch,\n",
    "                                              ResizePaddedTorch,\n",
    "                                              Compose,\n",
    "                                              DictToTensor,\n",
    "                                              ColorJitterTorch,\n",
    "                                              RandomRotations)\n",
    "\n",
    "from models.deeplabv3plus import DeepLabV3Plus\n",
    "from evaluation_metrics import Metrics, timer\n",
    "from visualizations import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory has been released.\n"
     ]
    }
   ],
   "source": [
    "# Release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU memory has been released.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Using device: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using device: CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = False\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "if SCALE:\n",
    "    aug = Compose([RandomChoice([RandomCropToSizeTorch(data_format='dict', size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    ResizePaddedTorch((0, 0), data_format='dict', size=(IMAGE_SIZE, IMAGE_SIZE))]),\n",
    "                    RandomRotations(format='cubi'),\n",
    "                    DictToTensor(),\n",
    "                    ColorJitterTorch()])\n",
    "else:\n",
    "    aug = Compose([RandomCropToSizeTorch(data_format='dict', size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                    RandomRotations(format='cubi'),\n",
    "                    DictToTensor(),\n",
    "                    ColorJitterTorch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  1000\n",
      "Validation set size:  400\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/cubicasa5k/'\n",
    "TRAIN_PATH = 'train.txt'\n",
    "VAL_PATH = 'val.txt'\n",
    "FORMAT = 'lmdb'\n",
    "\n",
    "full_train_set = FloorplanSVG(DATA_PATH, \n",
    "                         TRAIN_PATH, \n",
    "                         format=FORMAT, \n",
    "                         augmentations=aug)\n",
    "\n",
    "# Use this in the meantime to prevent kernel dying\n",
    "train_set = Subset(full_train_set, list(range(1000)))\n",
    "\n",
    "val_set = FloorplanSVG(DATA_PATH, \n",
    "                       VAL_PATH, \n",
    "                       format=FORMAT, \n",
    "                       augmentations=DictToTensor())\n",
    "\n",
    "print('Train set size: ', len(train_set))\n",
    "print('Validation set size: ', len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  tensor([[[1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         ...,\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886]],\n",
      "\n",
      "        [[1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         ...,\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886]],\n",
      "\n",
      "        [[1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         ...,\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886],\n",
      "         [1.1886, 1.1886, 1.1886,  ..., 1.1886, 1.1886, 1.1886]]])\n",
      "Label:  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "sample = train_set[0]\n",
    "print('Image: ', sample['image'])\n",
    "print('Label: ', sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([3, 256, 256])\n",
      "Label shape:  torch.Size([23, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print('Image shape: ', sample['image'].shape)\n",
    "print('Label shape: ', sample['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataloader: 50 batches of size 20\n",
      "Length of val dataloader: 20 batches of size 20\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "train_loader = DataLoader(train_set, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_workers=NUM_WORKERS, \n",
    "                          shuffle=True, \n",
    "                        #   pin_memory=True\n",
    "                          )\n",
    "\n",
    "val_loader = DataLoader(val_set, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        num_workers=NUM_WORKERS, \n",
    "                        # pin_memory=True\n",
    "                        )\n",
    "\n",
    "print(f'Length of train dataloader: {len(train_loader)} batches of size {BATCH_SIZE}')\n",
    "print(f'Length of val dataloader: {len(val_loader)} batches of size {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "For reference, here are the 23 classes:  \n",
    "\n",
    "- **Rooms (12):** \"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"  \n",
    "\n",
    "- **Icons (11):** \"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\", \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "DeepLabV3Plus                                                [20, 23, 256, 256]        --\n",
       "├─Backbone: 1-1                                              [20, 24, 64, 64]          --\n",
       "│    └─MobileNetV2: 2-1                                      --                        1,281,000\n",
       "│    │    └─Sequential: 3-1                                  --                        2,223,872\n",
       "├─ASPP: 1-2                                                  [20, 256, 8, 8]           --\n",
       "│    └─Conv2d: 2-2                                           [20, 256, 8, 8]           327,680\n",
       "│    └─AtrousConv: 2-3                                       [20, 256, 8, 8]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-2                      [20, 256, 8, 8]           339,712\n",
       "│    └─AtrousConv: 2-4                                       [20, 256, 8, 8]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-3                      [20, 256, 8, 8]           339,712\n",
       "│    └─AtrousConv: 2-5                                       [20, 256, 8, 8]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-4                      [20, 256, 8, 8]           339,712\n",
       "│    └─Sequential: 2-6                                       [20, 256, 1, 1]           --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-5                           [20, 1280, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-6                                      [20, 256, 1, 1]           327,680\n",
       "│    │    └─BatchNorm2d: 3-7                                 [20, 256, 1, 1]           512\n",
       "│    │    └─ReLU: 3-8                                        [20, 256, 1, 1]           --\n",
       "│    └─Sequential: 2-7                                       [20, 256, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-9                                      [20, 256, 8, 8]           327,680\n",
       "│    │    └─BatchNorm2d: 3-10                                [20, 256, 8, 8]           512\n",
       "│    │    └─ReLU: 3-11                                       [20, 256, 8, 8]           --\n",
       "├─Decoder: 1-3                                               [20, 23, 256, 256]        --\n",
       "│    └─Sequential: 2-8                                       [20, 48, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-12                                     [20, 48, 64, 64]          1,152\n",
       "│    │    └─BatchNorm2d: 3-13                                [20, 48, 64, 64]          96\n",
       "│    │    └─ReLU: 3-14                                       [20, 48, 64, 64]          --\n",
       "│    └─Sequential: 2-9                                       [20, 23, 64, 64]          --\n",
       "│    │    └─DepthwiseSeparableConv: 3-15                     [20, 256, 64, 64]         81,072\n",
       "│    │    └─DepthwiseSeparableConv: 3-16                     [20, 256, 64, 64]         68,352\n",
       "│    │    └─Conv2d: 3-17                                     [20, 23, 64, 64]          5,911\n",
       "==============================================================================================================\n",
       "Total params: 5,664,655\n",
       "Trainable params: 5,664,655\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 22.71\n",
       "==============================================================================================================\n",
       "Input size (MB): 15.73\n",
       "Forward/backward pass size (MB): 3970.25\n",
       "Params size (MB): 17.53\n",
       "Estimated Total Size (MB): 4003.52\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 23\n",
    "\n",
    "model = DeepLabV3Plus(backbone='mobilenetv2', attention=False, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL TRAINING\n",
    "EPOCHS = 50\n",
    "CRITERION = torch.nn.CrossEntropyLoss()\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 100\n",
    "\n",
    "# CRITERION = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# initial_lr = 0.001\n",
    "# OPTIMIZER = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.95, weight_decay=1e-4, nesterov=True)\n",
    "\n",
    "# # Poly learning rate policy (used in DeepLabV3+ paper)\n",
    "# class PolyLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "#     def __init__(self, optimizer, max_iters, power=0.9, last_epoch=-1):\n",
    "#         self.max_iters = max_iters\n",
    "#         self.power = power\n",
    "#         super(PolyLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "#     def get_lr(self):\n",
    "#         return [base_lr * (1 - self.last_epoch / self.max_iters) ** self.power for base_lr in self.base_lrs]\n",
    "\n",
    "# max_iters = EPOCHS * len(train_loader)\n",
    "# SCHEDULER = PolyLR(OPTIMIZER, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training process...\n",
      "Epoch 1 train process is started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:48<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation process is started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 619, 1319] at entry 0 and [3, 865, 489] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 138\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_iou\u001b[39m\u001b[38;5;124m'\u001b[39m: train_iou, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_pa\u001b[39m\u001b[38;5;124m'\u001b[39m: train_pixel_acc,\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_iou\u001b[39m\u001b[38;5;124m'\u001b[39m: val_iou, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_pa\u001b[39m\u001b[38;5;124m'\u001b[39m : val_pixel_acc}\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Training setup\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCRITERION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOPTIMIZER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# SCHEDULER,\u001b[39;49;00m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeeplabv3plus_cubicasa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs, num_classes, loss_fn, optimizer, device, early_stop_threshold, save_prefix, save_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m val_pixel_acc_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(val_loader)):\n\u001b[1;32m     77\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     78\u001b[0m         gts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 619, 1319] at entry 0 and [3, 865, 489] at entry 1"
     ]
    }
   ],
   "source": [
    "def train(model, \n",
    "          train_loader, \n",
    "          val_loader, \n",
    "          epochs,\n",
    "          num_classes,\n",
    "          loss_fn, \n",
    "          optimizer, \n",
    "        #   scheduler,\n",
    "          device,\n",
    "          early_stop_threshold=10,\n",
    "          save_prefix='deeplabv3plus',\n",
    "          save_path='saved_models'):\n",
    "\n",
    "    train_loss = []\n",
    "    train_pixel_acc = [] \n",
    "    train_iou = []\n",
    "    \n",
    "    val_loss = []\n",
    "    val_pixel_acc = []\n",
    "    val_iou = []\n",
    "    \n",
    "    train_len = len(train_loader)\n",
    "    val_len = len(val_loader)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    not_improve = 0\n",
    "    early_stop_threshold = early_stop_threshold\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Training loop\n",
    "    train_start = timer()\n",
    "    print('Start training process...')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tic = timer()\n",
    "        train_loss_ = 0\n",
    "        train_iou_ = 0 \n",
    "        train_pixel_acc_ = 0\n",
    "\n",
    "        print(f'Epoch {epoch} train process is started...')\n",
    "        model.train()\n",
    "        \n",
    "        for idx, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            imgs = batch['image']\n",
    "            gts = batch['label']\n",
    "            imgs, gts = imgs.to(device), gts.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear the gradients first\n",
    "            \n",
    "            preds = model(imgs)\n",
    "\n",
    "            train_metrics = Metrics(preds, gts, loss_fn, num_classes=num_classes)\n",
    "            loss = train_metrics.loss()\n",
    "            train_iou_ += train_metrics.mIOU()\n",
    "            train_pixel_acc_ += train_metrics.PixelAcc()\n",
    "            train_loss_ += loss.item()\n",
    "\n",
    "            loss.backward()  # Backpropagate current batch loss\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "        # Update learning rate using PolyLR\n",
    "        # scheduler.step()\n",
    "        \n",
    "        # Validation loop\n",
    "        print(f'Epoch {epoch} validation process is started...')\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss_ = 0\n",
    "        val_iou_ = 0\n",
    "        val_pixel_acc_ = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(tqdm(val_loader)):\n",
    "\n",
    "                imgs = batch['image']\n",
    "                gts = batch['label']\n",
    "                imgs, gts = imgs.to(device), gts.to(device)\n",
    "                \n",
    "                preds = model(imgs)\n",
    "\n",
    "                val_metrics = Metrics(preds, gts, loss_fn, num_classes=num_classes)\n",
    "                val_loss_ += val_metrics.loss().item()\n",
    "                val_iou_ += val_metrics.mIOU()\n",
    "                val_pixel_acc_ += val_metrics.PixelAcc()\n",
    "\n",
    "        print(f'Epoch {epoch} train process is completed.')\n",
    "\n",
    "        train_loss_ /= train_len\n",
    "        train_iou_ /= train_len\n",
    "        train_pixel_acc_ /= train_len\n",
    "\n",
    "        val_loss_ /= val_len\n",
    "        val_iou_ /=  val_len\n",
    "        val_pixel_acc_ /=   val_len\n",
    "\n",
    "        print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print(f'\\nEpoch {epoch} train process results: \\n')\n",
    "        print(f'Train Time         -> {timer(tic):.3f} secs')\n",
    "        print(f'Train Loss         -> {train_loss_:.3f}')\n",
    "        print(f'Train PA           -> {train_pixel_acc_:.3f}')\n",
    "        print(f'Train IoU          -> {train_iou_:.3f}')\n",
    "        print(f'Validation Loss    -> {val_loss_:.3f}')\n",
    "        print(f'Validation PA      -> {val_pixel_acc_:.3f}')\n",
    "        print(f'Validation IoU     -> {val_iou_:.3f}')\n",
    "        # print(f'Current LR         -> {scheduler.get_last_lr()[0]:.6f}\\n')\n",
    "\n",
    "        train_loss.append(train_loss_)\n",
    "        train_iou.append(train_iou_)\n",
    "        train_pixel_acc.append(train_pixel_acc_)\n",
    "\n",
    "        val_loss.append(val_loss_)\n",
    "        val_iou.append(val_iou_)\n",
    "        val_pixel_acc.append(val_pixel_acc_)\n",
    "\n",
    "        if val_loss_ < best_loss:\n",
    "            print(f'Loss decreased from {best_loss:.3f} to {val_loss_:.3f}!')\n",
    "            best_loss = val_loss_\n",
    "            not_improve = 0  # Reset counter\n",
    "            print('Saving the model with the best loss value...')\n",
    "            torch.save(model.state_dict(), f'{save_path}/{save_prefix}_best_model.pt')\n",
    "        else:\n",
    "            not_improve += 1\n",
    "            print(f'Loss did not decrease for {not_improve} epoch(s)!')\n",
    "            if not_improve >= early_stop_threshold:\n",
    "                print(f'Stopping training process because loss did not decrease for {early_stop_threshold} epochs!')\n",
    "                break\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n')\n",
    "\n",
    "    print(f'Train process is completed in {(timer(train_start)) / 60:.3f} minutes.')\n",
    "\n",
    "    return {'tr_loss': train_loss, 'tr_iou': train_iou, 'tr_pa': train_pixel_acc,\n",
    "            'val_loss': val_loss, 'val_iou': val_iou, 'val_pa' : val_pixel_acc}\n",
    "\n",
    "\n",
    "# Training setup\n",
    "history = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    EPOCHS,\n",
    "    NUM_CLASSES,\n",
    "    CRITERION,\n",
    "    OPTIMIZER,\n",
    "    # SCHEDULER,\n",
    "    device,\n",
    "    save_prefix='deeplabv3plus_cubicasa',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Test():\n",
    "#     def __init__(self, model, test_loader, loss_fn, device):\n",
    "#         self.model = model\n",
    "#         self.test_loader = test_loader\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.device = device\n",
    "    \n",
    "#     def run(self):\n",
    "#         self.model.eval()\n",
    "#         test_loss = 0\n",
    "#         test_iou = 0\n",
    "#         test_pixel_acc = 0\n",
    "#         test_len = len(self.test_loader)\n",
    "\n",
    "#         imgs = []\n",
    "#         gts = []\n",
    "#         preds = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(self.test_loader):\n",
    "#                 imgs_batch = batch['image']\n",
    "#                 gts_batch = batch['label']\n",
    "#                 imgs_batch, gts_batch = imgs_batch.to(self.device), gts_batch.to(self.device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 preds_batch = self.model(imgs_batch)\n",
    "                \n",
    "#                 # Calculate metrics\n",
    "#                 metrics = Metrics(preds_batch, gts_batch, self.loss_fn)\n",
    "#                 test_loss += metrics.loss().item()\n",
    "#                 test_iou += metrics.mIOU()\n",
    "#                 test_pixel_acc += metrics.PixelAcc()\n",
    "\n",
    "#                 # Collect data for visualization\n",
    "#                 preds_batch = torch.argmax(preds_batch, dim=1)\n",
    "#                 imgs.extend(imgs_batch.cpu())\n",
    "#                 gts.extend(gts_batch.cpu())\n",
    "#                 preds.extend(preds_batch.cpu())\n",
    "\n",
    "#         # Calculate average metrics\n",
    "#         test_loss /= test_len\n",
    "#         test_iou /= test_len\n",
    "#         test_pixel_acc /= test_len\n",
    "\n",
    "#         return imgs, gts, preds, test_loss, test_iou, test_pixel_acc\n",
    "\n",
    "\n",
    "# test = Test(model, test_loader, CRITERION, device)\n",
    "# imgs, gts, preds, test_loss, test_iou, test_pixel_acc = test.run()\n",
    "\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test mIoU: {test_iou:.4f}\")\n",
    "# print(f\"Test PA: {test_pixel_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
